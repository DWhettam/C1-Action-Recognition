import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import torch
import yaml

from utils.actions import action_id_from_verb_noun
from utils.metrics import compute_metrics
from utils.scoring import compute_action_scores

parser = argparse.ArgumentParser(
    description="Evaluate model results on the validation set",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument(
    "results", type=Path, help="Results generated by test.py (pth file)"
)
parser.add_argument("labels", type=Path, help="Labels (pickled dataframe)")
parser.add_argument(
    "--tail-verb-classes-csv", type=Path, default="metadata/tail_verb_classes.csv"
)
parser.add_argument(
    "--tail-noun-classes-csv", type=Path, default="metadata/tail_noun_classes.csv"
)
parser.add_argument(
    "--unseen-participant-ids-csv",
    type=Path,
    default="metadata/unseen_participants_validation.csv",
)


def collate(results):
    return {k: [r[k] for r in results] for k in results[0].keys()}


def add_action_class_column(groundtruth_df):
    groundtruth_df["action_class"] = action_id_from_verb_noun(
        groundtruth_df["verb_class"], groundtruth_df["noun_class"]
    )
    return groundtruth_df


def main(args):
    labels: pd.DataFrame = pd.read_pickle(args.labels)
    if "narration_id" in labels.columns:
        labels.set_index("narration_id", inplace=True)
    labels = add_action_class_column(labels)
    unseen_participants: np.ndarray = pd.read_csv(
        args.unseen_participant_ids_csv, index_col="participant_id"
    ).index.values
    tail_verb_classes: np.ndarray = pd.read_csv(
        args.tail_verb_classes_csv, index_col="verb"
    ).index.values
    tail_noun_classes: np.ndarray = pd.read_csv(
        args.tail_noun_classes_csv, index_col="noun"
    ).index.values

    results = collate(
        torch.load(args.results, map_location=lambda storage, loc: storage)
    )
    verb_output = np.array(results["verb_output"])
    noun_output = np.array(results["noun_output"])
    narration_ids = np.array(results["narration_id"])
    scores = {
        "verb": verb_output,
        "noun": noun_output,
    }
    (verbs, nouns), _scores = compute_action_scores(
        scores["verb"], scores["noun"], top_n=100
    )
    scores["action"] = [
        {
            action_id_from_verb_noun(verb, noun): score
            for verb, noun, score in zip(segment_verbs, segment_nouns, segment_score)
        }
        for segment_verbs, segment_nouns, segment_score in zip(verbs, nouns, _scores)
    ]
    accuracies = compute_metrics(
        labels.loc[narration_ids],
        scores,
        tail_verb_classes,
        tail_noun_classes,
        unseen_participants,
    )

    display_metrics = dict()
    for split in accuracies.keys():
        for task in ["verb", "noun", "action"]:
            task_accuracies = accuracies[split][task]
            for k, task_accuracy in zip((1, 5), task_accuracies):
                display_metrics[f"{split}_{task}_accuracy_at_{k}"] = task_accuracy
    display_metrics = {metric: float(value * 100)
                       for metric, value in display_metrics.items()}

    print(yaml.dump(display_metrics))


if __name__ == "__main__":
    main(parser.parse_args())
